# Example configuration for running an experiment using the IWSLT-15 dataset.

include "iwslt15.conf"

data {
  train-batch-size: 128
  infer-batch-size: 32
  eval-batch-size: 32

  vocabulary {
    type: "word-count"
    shared: false
    size: 20000
    min-count: 5
  }
}

model {
  parameters {
    word-embeddings-size: 512
  }

  encoder {
    type: "bi-rnn"
    num-units: ${model.parameters.word-embeddings-size}
    num-layers: 2
    residual: true
    dropout: 0.2

    cell {
      type: "lstm"
      activation: "tanh"
      forget-bias: 1.0
    }
  }

  decoder {
    type: "rnn"
    num-units: ${model.parameters.word-embeddings-size}
    num-layers: 2
    residual: true
    dropout: 0.2
    use-attention: true

    cell {
      type: "lstm"
      activation: "tanh"
      forget-bias: 1.0
    }
  }

  training {
    use-identity-translations: true
    checkpoint-steps: 1000
    summary-steps: 100

    optimization {
      optimizer: "amsgrad"
      learning-rate: 0.001
      max-grad-norm: 10.0
    }
  }
}
