# Example configuration for running an experiment using the IWSLT-15 dataset.

include "iwslt15.conf"

data {
  train-batch-size: 128
  infer-batch-size: 32
  eval-batch-size: 32

  vocabulary {
    type: "word-count"
    shared: false
    size: 20000
    min-count: 5
  }
}

model {
  languages: "en:vi"
  eval-languages: "en:vi"
  both-directions: false

  parameters {
    word-embeddings-size: 512
    manager: "pairwise"
  }

  encoder {
    type: "transformer"
    num-units: ${model.parameters.word-embeddings-size}
    num-layers: 6
    attention-keys-depth: ${model.parameters.word-embeddings-size}
    attention-values-depth: ${model.parameters.word-embeddings-size}
    attention-num-heads: 8
  }

  decoder {
    type: "transformer"
    num-layers: 6
    attention-keys-depth: ${model.parameters.word-embeddings-size}
    attention-values-depth: ${model.parameters.word-embeddings-size}
    attention-num-heads: 8
  }

  training {
    use-identity-translations: false
    checkpoint-steps: 1000
    summary-steps: 100

    optimization {
      optimizer: "amsgrad"
      learning-rate: 0.001
      max-grad-norm: 10.0
    }
  }

  inference {
    beam-width: 1
  }
}
