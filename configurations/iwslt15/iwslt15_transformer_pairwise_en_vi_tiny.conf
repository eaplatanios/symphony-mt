# Example configuration for running an experiment using the IWSLT-15 dataset.

include "iwslt15.conf"

data {
  train-batch-size: 32
  infer-batch-size: 32
  eval-batch-size: 32

  vocabulary {
    type: "bpe"
    case-sensitive: false
    shared: true
    num-merge-ops: 5000
    min-count: 5
  }
}

model {
  languages: "en:vi"
  eval-languages: "en:vi"
  both-directions: false

  parameters {
    word-embeddings-size: 128
    manager: "pairwise"
  }

  encoder {
    type: "transformer"
    num-units: ${model.parameters.word-embeddings-size}
    num-layers: 3
    attention-keys-depth: 32 # ${model.parameters.word-embeddings-size}
    attention-values-depth: 32 # ${model.parameters.word-embeddings-size}
    attention-num-heads: 4
  }

  decoder {
    type: "transformer"
    num-layers: 3
    attention-keys-depth: 32 # ${model.parameters.word-embeddings-size}
    attention-values-depth: 32 # ${model.parameters.word-embeddings-size}
    attention-num-heads: 4
  }

  training {
    use-identity-translations: false
    checkpoint-steps: 1000
    summary-steps: 100

    optimization {
      optimizer: "amsgrad"
      learning-rate: 0.0001
      max-grad-norm: 100.0
    }
  }

  inference {
    beam-width: 1
  }
}
