include "base.conf"

data {
  train-batch-size: 512
  infer-batch-size: 128
  eval-batch-size: 128

  include "../../../vocabularies/bpe-shared-32k.conf"
}

model {
  name: "transformer-base-512"

  parameters {
    word-embeddings-size: 512

    include "../../../parameters/pairwise.conf"
  }

  include "../../../models/transformer/transformer-base.conf"
}

training {
  checkpoint-frequency: 10000

  optimization {
    learning-rate {
      value: 0.001

      schedule {
        type: "noam"
        warmup-steps: 10000
        hidden-size: ${model.encoder.num-units}
      }
    }
  }
}

inference {
  beam-width: 4
  length-penalty: 0.6
  max-decoding-length-factor: 2.0
}

evaluation {
  frequency: 10000
}
