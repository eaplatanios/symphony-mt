include "base.conf"

data {
  train-batch-size: 32
  infer-batch-size: 128
  eval-batch-size: 128

  include "../../../vocabularies/bpe-shared-5k.conf"
}

model {
  name: "transformer-tiny-128-32"

  parameters {
    word-embeddings-size: 32

    include "../../../parameters/pairwise.conf"
  }

  include "../../../models/transformer/transformer-tiny.conf"

  encoder {
    num-units: 128
  }

  decoder {
    num-units: 128
  }
}

training {
  checkpoint-frequency: 5000

  optimization {
    optimizer: "adaptive_amsgrad"
  }
}

evaluation {
  frequency: 5000
}
