include "base.conf"

data {
  train-batch-size: 512
  infer-batch-size: 128
  eval-batch-size: 128

  include "../../../vocabularies/bpe-shared-32k.conf"
}

model {
  name: "transformer-base-512"

  parameters {
    word-embeddings-size: 512

    include "../../../parameters/pairwise.conf"
  }

  include "../../../models/transformer/transformer-base.conf"
}

training {
  checkpoint-frequency: 1000

  optimization {
    max-grad-norm: 10.0
  }
}

inference {
  beam-width: 4
  length-penalty: 0.6
  max-decoding-length-factor: 2.0
}

evaluation {
  frequency: 1000
}
